{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from discriminator import Discriminator\n",
    "from generator import Generator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from utils import *\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from datasets import *\n",
    "\n",
    "from utils import *\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "# init dataset\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "])\n",
    "TRAIN_DIR = \"/ssd_scratch/cvit/anirudhkaushik/datasets/cyclegan/apple2orange/apple2orange/\"\n",
    "VAL_DIR = \"/ssd_scratch/cvit/anirudhkaushik/datasets/cyclegan/apple2orange/apple2orange/\"\n",
    "\n",
    "dataset = AppleDataset(\n",
    "    root_apple=TRAIN_DIR + \"/trainA\",\n",
    "    transform=data_transforms,\n",
    ")\n",
    "val_dataset = AppleDataset(\n",
    "    root_apple=VAL_DIR + \"/testA\",\n",
    "    transform=data_transforms,\n",
    ")\n",
    "\n",
    "dataset2 = OrangeDataset(\n",
    "    root_orange=TRAIN_DIR + \"/trainB\",\n",
    "    transform=data_transforms,\n",
    ")\n",
    "val_dataset2 = OrangeDataset(\n",
    "    root_orange=VAL_DIR + \"/testB\",\n",
    "    transform=data_transforms,\n",
    ")\n",
    "\n",
    "dataset = torch.utils.data.ConcatDataset([dataset, val_dataset])\n",
    "dataset2 = torch.utils.data.ConcatDataset([dataset2, val_dataset2])\n",
    "\n",
    "\n",
    "def generate_real_images( n_samples, patch_shape):\n",
    "    y = np.ones((n_samples, 1, patch_shape, patch_shape))\n",
    "    y = torch.FloatTensor(y)\n",
    "    return  y\n",
    "\n",
    "def generate_fake_images(model, dataset, patch_shape ):\n",
    "    X = model(dataset)\n",
    "    y = np.zeros((len(X), 1, patch_shape, patch_shape, ))\n",
    "    y = torch.FloatTensor(y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize(dataloader, dataloader2,  netG, netD, criterion, epoch, fixed_noise, device):\n",
    "    real1 = dataloader\n",
    "    real2 = dataloader2\n",
    "\n",
    "    for i, (X_real1, X_real2) in enumerate(zip(real1, real2)):\n",
    "        break\n",
    "\n",
    "    \n",
    "    X_real1 = X_real1.to(device)\n",
    "    X_real2 = X_real2.to(device)\n",
    "    \n",
    "    # fake, _ = generate_fake_images(netG, X_real1, 1)\n",
    "    fake,_ = generate_fake_images(netG, X_real1, 1)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Plot the fake images from the last epoch\n",
    "\n",
    "    # fake = (fake - fake.min())/ (fake.max() - fake.min())\n",
    "    # for row in range(8):\n",
    "    #     for col in range(8):\n",
    "    #         plt.subplot(8, 8, row*8+col+1)\n",
    "    #         plt.axis(\"off\")\n",
    "    #         # set subplot label to be the label of the image\n",
    "    #         plt.title(y_labels[row*8+col].item())\n",
    "    #         # make title fit properly\n",
    "    #         plt.subplots_adjust(top=1.5)\n",
    "    #         # correct for mean 0 and std 1\n",
    "    #         # unnormalize\n",
    "    #         # adjust to 0 to 1 range\n",
    "    #         plt.imshow(fake[row*8+col].detach().cpu().numpy().transpose(1,2,0), cmap='gray')\n",
    "\n",
    "\n",
    "    return X_real1, fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 256\n",
    "fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "img_list = []\n",
    "import glob\n",
    "for checkpointD1, checkpointG1, checkpointD2, checkpointG2 in sorted(list(zip(glob.glob(\"/ssd_scratch/cvit/anirudhkaushik/checkpoints/cycle_ganD1_checkpoint_*.pt\"),glob.glob(\"/ssd_scratch/cvit/anirudhkaushik/checkpoints/cycle_ganG1_checkpoint_*.pt\"), glob.glob(\"/ssd_scratch/cvit/anirudhkaushik/checkpoints/cycle_ganD2_checkpoint_*.pt\"),glob.glob(\"/ssd_scratch/cvit/anirudhkaushik/checkpoints/cycle_ganG2_checkpoint_*.pt\")))):\n",
    "    print(\"epoch :\" + checkpointD1.split(\"_\")[-2].split(\".\")[0])\n",
    "    \n",
    "    checkpointD1 = torch.load(checkpointD1)\n",
    "    checkpointG1 = torch.load(checkpointG1)\n",
    "    checkpointD2 = torch.load(checkpointD2)\n",
    "    checkpointG2 = torch.load(checkpointG2)\n",
    "\n",
    "    modelG1 = Generator(256, 3)\n",
    "    modelD1 = Discriminator(3)\n",
    "    modelG1.load_state_dict(checkpointG1['model'])\n",
    "    modelD1.load_state_dict(checkpointD1['model'])\n",
    "\n",
    "    modelG1 = modelG1.to(device)\n",
    "    modelD1 = modelD1.to(device)\n",
    "\n",
    "    modelG2 = Generator(256, 3)\n",
    "    modelD2 = Discriminator(3)\n",
    "    modelG2.load_state_dict(checkpointG2['model'])\n",
    "    modelD2.load_state_dict(checkpointD2['model'])\n",
    "\n",
    "    modelG2 = modelG2.to(device)\n",
    "    modelD2 = modelD2.to(device)\n",
    "\n",
    "    # visualize\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    real, fake = visualize(dataloader, dataloader2, modelG1, modelD1, criterion, 0, fixed_noise, device)\n",
    "    real2, fake2 = visualize(dataloader2, dataloader, modelG2, modelD2, criterion, 0, fixed_noise, device)\n",
    "    # img_list.append(vutils.make_grid(torch.concat((real, fake, real2, fake2), dim=0 ), padding=0, normalize=False, nrow=2, scale_each=False, pad_value=0.5))\n",
    "    # add title of epoch to the images of the grid\n",
    "    img_list.append(vutils.make_grid(torch.cat((real, fake, real2, fake2), dim=0 ), padding=0, normalize=True, nrow=2, scale_each=False))\n",
    "    # img_list.append(vutils.make_grid(torch.concat((real, fake, real2, fake2), dim=0 ), padding=0, normalize=False, nrow=2, scale_each=False, pad_value=0.5))\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "for i in range(len(img_list)):\n",
    "    img_list[i] = img_list[i].cpu().detach().numpy()\n",
    "# add title epoch to the images of the grid\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list[0:]]\n",
    "# add title epoch to the images of the grid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fake.shape)\n",
    "score = modelD1(real)\n",
    "print(score.mean().item())\n",
    "score = modelD1(fake)\n",
    "print(score.mean().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelD1 = Discriminator(3)\n",
    "checkpointD1 = torch.load(\"/ssd_scratch/cvit/anirudhkaushik/checkpoints/cycle_ganD1_checkpoint_latest.pt\")\n",
    "modelD1.load_state_dict(checkpointD1['model'])\n",
    "modelD1 = modelD1.to(device)\n",
    "\n",
    "modelG1 = Generator(256, 3)\n",
    "checkpointG1 = torch.load(\"/ssd_scratch/cvit/anirudhkaushik/checkpoints/cycle_ganG1_checkpoint_latest.pt\")\n",
    "modelG1.load_state_dict(checkpointG1['model'])\n",
    "modelG1 = modelG1.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for real1, real2 in zip(dataloader, dataloader2):\n",
    "    break\n",
    "\n",
    "real1 = real1.to(device)\n",
    "real2 = real2.to(device)\n",
    "\n",
    "fake_zebra = modelG1(real1)\n",
    "score = modelD1(fake_zebra.detach())\n",
    "\n",
    "real1 = (real1-real1.min())/(real1.max()-real1.min())\n",
    "print(score.mean().item())\n",
    "score_real = modelD1(real2)\n",
    "print(score_real.mean().item())\n",
    "\n",
    "plt.imshow(fake_zebra.squeeze(0).detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(real1.squeeze(0).detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fake.squeeze(0).detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(real.squeeze(0).detach().cpu().numpy().transpose(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
